/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: evaluate.py [-h] --alg_name
                   {AlphaEdit,MEMIT_rect,MEMIT_seq,MEMIT_prune,MEMIT,ROME,FT,MEND,NSE}
                   --model_name MODEL_NAME --hparams_fname HPARAMS_FNAME
                   [--ds_name {mcf,cf,zsre,mquake}]
                   [--continue_from_run CONTINUE_FROM_RUN]
                   [--dataset_size_limit DATASET_SIZE_LIMIT]
                   [--skip_generation_tests]
                   [--generation_test_interval GENERATION_TEST_INTERVAL]
                   [--conserve_memory] [--num_edits NUM_EDITS] [--use_cache]
                   [--downstream_eval_steps DOWNSTREAM_EVAL_STEPS]
evaluate.py: error: unrecognized arguments:  
Traceback (most recent call last):
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/experiments/evaluate.py", line 26, in <module>
    from memit import MEMITHyperParams
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/memit/__init__.py", line 1, in <module>
    from .memit_main import MEMITHyperParams, apply_memit_to_model
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/memit/memit_main.py", line 10, in <module>
    from rome.layer_stats import layer_stats
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/rome/__init__.py", line 1, in <module>
    from .rome_main import ROMEHyperParams, apply_rome_to_model, execute_rome
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/rome/rome_main.py", line 12, in <module>
    from .compute_u import compute_u
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/rome/compute_u.py", line 8, in <module>
    from rome import repr_tools
  File "/root/autodl-tmp/sic_lyu/AlphaEdit/rome/repr_tools.py", line 11, in <module>
    from transformers.models.gptj.modeling_gptj import GPTJForCausalLM
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py", line 38, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/transformers/modeling_utils.py", line 61, in <module>
    from .integrations.flex_attention import flex_attention_forward
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/transformers/integrations/flex_attention.py", line 45, in <module>
    class WrappedFlexAttention:
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/transformers/integrations/flex_attention.py", line 60, in WrappedFlexAttention
    @torch.compiler.disable(recursive=False)
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/compiler/__init__.py", line 226, in disable
    import torch._dynamo
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/__init__.py", line 39, in <module>
    from .polyfills import loader as _  # usort: skip # noqa: F401
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/polyfills/loader.py", line 22, in <module>
    POLYFILLED_MODULES: Tuple["ModuleType", ...] = tuple(
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/polyfills/loader.py", line 23, in <genexpr>
    importlib.import_module(f".{submodule}", package=polyfills.__name__)
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/polyfills/builtins.py", line 24, in <module>
    def all(iterable: Iterable[object], /) -> bool:
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/decorators.py", line 312, in wrapper
    rule_map: Dict[Any, Type[VariableTracker]] = get_torch_obj_rule_map()
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py", line 2860, in get_torch_obj_rule_map
    obj = load_object(k)
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py", line 2891, in load_object
    val = _load_obj_from_str(x[0])
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py", line 2875, in _load_obj_from_str
    return getattr(importlib.import_module(module), obj_name)
  File "/root/autodl-tmp/miniconda3/envs/alphaedit/lib/python3.10/importlib/__init__.py", line 117, in import_module
    if name.startswith('.'):
KeyboardInterrupt
